{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140580ef-5db0-43cc-a524-9c39e04d4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain unstructured[all-docs] pydantic lxml openai chromadb tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b56bde-1ba0-4525-a11d-cab02c5659e4",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "### Partition PDF tables, text, and images\n",
    "\n",
    "* Use [Unstructured](https://unstructured-io.github.io/unstructured/) to partition elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4e0614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\anaconda\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in d:\\anaconda\\lib\\site-packages (from pytesseract) (10.2.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91eeec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf1b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98bdeb7-eb77-42e6-a3a5-c3f27a1838d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "input_path = os.getcwd()\n",
    "output_path = os.path.join(os.getcwd(), \"output\")\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=os.path.join(input_path, \"test.pdf\"),\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f660305-e165-4b6c-ada3-a67a422defb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "text_elements = []\n",
    "table_elements = []\n",
    "image_elements = []\n",
    "\n",
    "# Function to encode images\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    if 'CompositeElement' in str(type(element)):\n",
    "        text_elements.append(element)\n",
    "    elif 'Table' in str(type(element)):\n",
    "        table_elements.append(element)\n",
    "\n",
    "table_elements = [i.text for i in table_elements]\n",
    "text_elements = [i.text for i in text_elements]\n",
    "\n",
    "# Tables\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "output_path = r\"./figures\"\n",
    "for image_file in os.listdir(output_path):\n",
    "    if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        image_path = os.path.join(output_path, image_file)\n",
    "        image_elements.append(image_path)\n",
    "print(len(image_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48154008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Summary: AI เป็นสิ่งที่แวดวงธุรกิจ และอุตสาหกรรมให้ความสนใจกันเป็นอย่างมาก แม้แต่คนทั่วไปก็อาจเคยได้ยินคำว่า AI กันมานาน บางคนรู้จัก AI ผ่านภาพยนตร์ ซีรีส์ หรือจากเกมต่างๆ ซึ่งจริงๆ แล้ว AI เป็นส่วนหนึ่งของชีวิตประจำวันของเรามากกว่าที่คิด แต่หลายคนอาจไม่รู้ว่าจริงๆ แล้ว AI คืออะไร ระบบ AI มีหลักการทำงานอย่างไร AI มีกี่ประเภทกันแน่ในปัจจุบัน ทำไมถึงมีความสำคัญต่อธุรกิจ และอุตสาหกรรมนัก หรือที่หลายคนสงสัยว่า AI สามารถคิด และมีความรู้สึกจริงไหม ซึ่งในบทความนี้ได้นำความรู้ต่างๆ เกี่ยวกับเทคโนโลยี AI ว่าคืออะไร AI เข้ามามีบทบาทในการช่วยให้การทำงานในแวดวงอุตสาหกรรมสะดวกสบายอย่างไร เพื่อให้เข้าใจความหมายของ AI และรู้จักการทำงานของ AI มากขึ้น\n",
      "Table Summary: The table contains two columns, Column1 and Column2, with two rows of data. The first row contains the data \"Data1\" in Column1 and \"Data2\" in Column2. The second row contains the data \"Data1\" in Column1 and \"Data2\" in Column2.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages and set API key\n",
    "from aift.multimodal import textqa\n",
    "from aift import setting\n",
    "\n",
    "# Set the API key\n",
    "setting.set_api_key('hChDEu87Ln2VThG8HVqEweoeVoVSGHgJ')\n",
    "\n",
    "# Function for text summaries\n",
    "def summarize_text(text_element):\n",
    "    session_id = 'YOUR_SESSION'\n",
    "    prompt = f\"Summarize the following text:\\n\\n{text_element}\\n\\nSummary:\"\n",
    "    response = textqa.chat(prompt, sessionid=session_id, temperature=0.2, return_json=False)\n",
    "    return response\n",
    "\n",
    "# Function for table summaries\n",
    "def summarize_table(table_element):\n",
    "    session_id = 'YOUR_SESSION'\n",
    "    prompt = f\"Summarize the following table:\\n\\n{table_element}\\n\\nSummary:\"\n",
    "    response = textqa.chat(prompt, sessionid=session_id, temperature=0.2, return_json=False)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "text_summary = summarize_text(\"AI เป็นสิ่งที่แวดวงธุรกิจ และอุตสาหกรรมให้ความสนใจกันเป็นอย่างมาก แม้แต่คนทั่วไปก็อาจเคยได้ยินคำว่า AI กันมานาน บางคนรู้จัก AI ผ่านภาพยนตร์ ซีรีส์ หรือจากเกมต่างๆ ซึ่งจริงๆ แล้ว AI เป็นส่วนหนึ่งของชีวิตประจำวันของเรามากกว่าที่คิด แต่หลายคนอาจไม่รู้ว่าจริงๆ แล้ว AI คืออะไร ระบบ AI มีหลักการทำงานอย่างไร AI มีกี่ประเภทกันแน่ในปัจจุบัน ทำไมถึงมีความสำคัญต่อธุรกิจ และอุตสาหกรรมนัก หรือที่หลายคนสงสัยว่า AI สามารถคิด และมีความรู้สึกจริงไหม ซึ่งในบทความนี้ได้นำความรู้ต่างๆ เกี่ยวกับเทคโนโลยี AI ว่าคืออะไร AI เข้ามามีบทบาทในการช่วยให้การทำงานในแวดวงอุตสาหกรรมสะดวกสบายอย่างไร เพื่อให้เข้าใจความหมายของ AI และรู้จักการทำงานของ AI มากขึ้น\")\n",
    "table_summary = summarize_table(\"Column1 | Column2\\n------|------\\nData1 | Data2\")\n",
    "print(\"Text Summary:\", text_summary)\n",
    "print(\"Table Summary:\", table_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47683f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'large-language-models-llms-process.jpg',\n",
       " 'content_type': 'image/jpeg',\n",
       " 'query': 'รูปนี้คืออะไร',\n",
       " 'content': '',\n",
       " 'execution_time': '0.48'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import package and set API_KEY\n",
    "from aift.multimodal import vqa\n",
    "from aift import setting\n",
    "\n",
    "\n",
    "vqa.generate(r'C:\\Users\\Admin\\Desktop\\LLM\\Multimodal-RAG-With-OpenAI\\large-language-models-llms-process.jpg', 'รูปนี้คืออะไร')\n",
    "# output\n",
    "# {'filename': '/path/to/your/image_file/car-demo.png',\n",
    "#  'content_type': 'image/jpeg',\n",
    "#  'query': 'รูปนี้คืออะไร',\n",
    "#  'content': 'รถยนต์คันสีแดงจอดอยู่ข้างกับกำแพงสีขาวมีลายเส้นสีน้ำตาล',\n",
    "#  'execution_time': '1.29'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa11e9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'figure-3-1.jpg', 'content_type': 'image/jpeg', 'query': 'Describe the contents of this image.', 'content': '', 'execution_time': '0.42'}\n",
      "Image Description: \n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages and set API key\n",
    "from aift.multimodal import vqa\n",
    "from aift import setting\n",
    "\n",
    "# Function for image summaries\n",
    "def summarize_image(image_path):\n",
    "    query = \"Describe the contents of this image.\"\n",
    "    response = vqa.generate(image_path, query)\n",
    "    print(response)\n",
    "    description = response.get('content', 'No description available')\n",
    "    execution_time = response.get('execution_time', 'N/A')\n",
    "    return description\n",
    "\n",
    "# Example usage\n",
    "image_path = r\"C:\\Users\\Admin\\Desktop\\LLM\\Multimodal-RAG-With-OpenAI\\figures\\figure-3-1.jpg\"  # Replace with actual image path\n",
    "image_description= summarize_image(image_path)\n",
    "print(\"Image Description:\", image_description)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th element of tables processed.\n",
      "2th element of tables processed.\n"
     ]
    }
   ],
   "source": [
    "# Processing table elements with feedback and sleep\n",
    "table_summaries = []\n",
    "for i, te in enumerate(table_elements[0:2]):\n",
    "    summary = summarize_table(te)\n",
    "    table_summaries.append(summary)\n",
    "    print(f\"{i + 1}th element of tables processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th element of texts processed.\n",
      "2th element of texts processed.\n"
     ]
    }
   ],
   "source": [
    "# Processing text elements with feedback and sleep\n",
    "text_summaries = []\n",
    "for i, te in enumerate(text_elements[0:2]):\n",
    "    summary = summarize_text(te)\n",
    "    text_summaries.append(summary)\n",
    "    print(f\"{i + 1}th element of texts processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b7c826e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./figures\\\\figure-14-5.jpg',\n",
       " './figures\\\\figure-15-6.jpg',\n",
       " './figures\\\\figure-18-10.jpg',\n",
       " './figures\\\\figure-18-11.jpg',\n",
       " './figures\\\\figure-18-12.jpg',\n",
       " './figures\\\\figure-18-13.jpg',\n",
       " './figures\\\\figure-18-14.jpg',\n",
       " './figures\\\\figure-18-15.jpg',\n",
       " './figures\\\\figure-18-16.jpg',\n",
       " './figures\\\\figure-18-17.jpg',\n",
       " './figures\\\\figure-18-7.jpg',\n",
       " './figures\\\\figure-18-8.jpg',\n",
       " './figures\\\\figure-18-9.jpg',\n",
       " './figures\\\\figure-19-18.jpg',\n",
       " './figures\\\\figure-19-19.jpg',\n",
       " './figures\\\\figure-19-20.jpg',\n",
       " './figures\\\\figure-19-21.jpg',\n",
       " './figures\\\\figure-19-22.jpg',\n",
       " './figures\\\\figure-3-1.jpg',\n",
       " './figures\\\\figure-4-2.jpg',\n",
       " './figures\\\\figure-7-3.jpg',\n",
       " './figures\\\\figure-8-4.jpg']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': './figures\\\\figure-14-5.jpg', 'content_type': 'image/jpeg', 'query': 'Describe the contents of this image.', 'content': '', 'execution_time': '0.43'}\n",
      "1th element of images processed.\n",
      "{'filename': './figures\\\\figure-15-6.jpg', 'content_type': 'image/jpeg', 'query': 'Describe the contents of this image.', 'content': '', 'execution_time': '0.37'}\n",
      "2th element of images processed.\n"
     ]
    }
   ],
   "source": [
    "# Processing image elements with feedback and sleep\n",
    "image_summaries = []\n",
    "for i, ie in enumerate(image_elements[0:2]):\n",
    "    summary = summarize_image(ie)\n",
    "    image_summaries.append(summary)\n",
    "    print(f\"{i + 1}th element of images processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6a90188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7f52f-bf5c-4ba4-af72-b2ccba59a4cf",
   "metadata": {},
   "source": [
    "## Multi-vector retriever\n",
    "\n",
    "Use [multi-vector-retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary).\n",
    "\n",
    "Summaries are used to retrieve raw tables and / or raw chunks of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b030d4-2ac5-41b6-9245-fc3ba5771d87",
   "metadata": {},
   "source": [
    "### Add to vectorstore\n",
    "\n",
    "Use [Multi Vector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary) with summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05303e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfad4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def __init__(self, model_name=\"mrp/simcse-model-m-bert-thai-cased\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding model using SentenceTransformer.\n",
    "        :param model_name: Name of the pre-trained model\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a single query.\n",
    "        :param text: Input text to embed\n",
    "        :return: Embedding vector as a Python list\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()  # Convert NumPy array to list\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        \"\"\"\n",
    "        Asynchronous version of `embed_query`.\n",
    "        :param text: Input text to embed\n",
    "        :return: Embedding vector as a Python list\n",
    "        \"\"\"\n",
    "        return self.embed_query(text)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for multiple documents.\n",
    "        :param texts: List of input texts to embed\n",
    "        :return: List of embedding vectors as Python lists\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return [embedding.tolist() for embedding in embeddings]\n",
    "\n",
    "    async def aembed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Asynchronous version of `embed_documents`.\n",
    "        :param texts: List of input texts to embed\n",
    "        :return: List of embedding vectors as Python lists\n",
    "        \"\"\"\n",
    "        return self.embed_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d643cc61-827d-4f3c-8242-7a7c8291ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initialize the vector store and storage layer\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=CustomEmbeddings(model_name=\"mrp/simcse-model-m-bert-thai-cased\"))\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key)\n",
    "\n",
    "# Function to add documents to the retriever\n",
    "def add_documents_to_retriever(summaries, original_contents):\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, original_contents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text summaries\n",
    "add_documents_to_retriever(text_summaries, text_elements)\n",
    "\n",
    "# Add table summaries\n",
    "add_documents_to_retriever(table_summaries, table_elements)\n",
    "\n",
    "# Add image summaries\n",
    "add_documents_to_retriever(image_summaries, image_summaries) # hopefully real images soon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45fb81-46b1-426e-aa2c-01aed4eac700",
   "metadata": {},
   "source": [
    "# Table retrieval\n",
    "\n",
    "The most complex table in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bea75fe-85af-4955-a80c-6e0b44a8e215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_25992\\144532207.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Conversation Detail description Complex reasoning All Full data 83.1 75.3 96.5 85.1 Detail + Complex 81.5 (-1.6) 73.3 (-2.0) 90.8 (-5.7) 81.9 (-3.2) Conv + 5% Detail + 10% Complex 81.0 (-2.1) 68.4 (-7.1) 91.5 (-5.0) 80.5 (-4.4) Conversation 76.5 (-6.6) 59.8 (-16.2 ) 84.9 (-12.4 ) 73.8 (-11.3 ) No Instruction Tuning 22.0 ( -61.1 ) 24.0 ( -51.3 ) 18.5 ( -78.0 ) 21.5 ( -63.6 )',\n",
       " 'Visual features Best variant Predict answer ﬁrst Training from scratch 7B model size Before 90.92 - 85.81 (-5.11) 89.84 (-1.08) Last 89.96 (-0.96) 89.77 (-1.15) - -']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can retrieve this table\n",
    "retriever.get_relevant_documents(\n",
    "    \"What do you see on the images in the database?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde6f17-d244-4270-b759-68e1858d399f",
   "metadata": {},
   "source": [
    "We can retrieve this image summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b789296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aift.multimodal import textqa\n",
    "from aift import setting\n",
    "\n",
    "# Set Pathumma API Key\n",
    "setting.set_api_key('API_KEY')\n",
    "\n",
    "# Define a simple wrapper for Pathumma\n",
    "class PathummaModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def generate(self, instruction: str, return_json: bool = False):\n",
    "        response = textqa.generate(instruction=instruction, return_json=return_json)\n",
    "        if return_json:\n",
    "            return response.get(\"content\", \"\")\n",
    "        return response\n",
    "\n",
    "    def __call__(self, input: str):\n",
    "        return self.generate(input, return_json=False)\n",
    "\n",
    "# Initialize Pathumma Model\n",
    "model_local = PathummaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a47fa-1267-4db8-a6ae-5fde48bbc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "after_rag_template = \"\"\"ตอบคำถามโดยพิจารณาจากบริบทต่อไปนี้เท่านั้น:\n",
    "{context}\n",
    "คำถาม: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(after_rag_template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea8414a8-65ee-4e11-8154-029b454f46af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the images in the database, you can see various visual features that are used to train a model. The best variant of these features is used to predict an answer first, and then training from scratch with a 7B model size before achieving certain performance metrics.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "     \"What do you see on the images in the database?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
