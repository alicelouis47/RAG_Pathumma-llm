{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mistal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before RAG\n",
      "\n",
      " Ollama is a large language model developed by Mistral AI, a company based in Paris, France. As of my last update, it was announced but not yet publicly released. The model is said to be a significant advancement in the field of artificial intelligence and could potentially compete with models like ChatGPT from OpenAI or Bard from Google. However, I don't have real-time updates, so please verify from a reliable source for the latest information.\n",
      "\n",
      "########\n",
      "After RAG\n",
      "\n",
      " Based on the provided text snippets, Ollama appears to be a company or platform that provides large language models. The text mentions various ways to interact with Ollama, such as through its blog, Discord, GitHub, and by downloading its models. The text also suggests that Ollama supports existing tooling built for OpenAI and has an API that can be accessed using PowerShell.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import embeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "model_local = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# 1. Split data into chunks\n",
    "urls = [\n",
    "    \"https://ollama.com/\",\n",
    "    \"https://ollama.com/blog/windows-preview\",\n",
    "    \"https://ollama.com/blog/openai-compatibility\",\n",
    "]\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# 2. Convert documents to Embeddings and store them\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings.OllamaEmbeddings(model='nomic-embed-text'),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. Before RAG\n",
    "print(\"Before RAG\\n\")\n",
    "before_rag_template = \"What is {topic}\"\n",
    "before_rag_prompt = ChatPromptTemplate.from_template(before_rag_template)\n",
    "before_rag_chain = before_rag_prompt | model_local | StrOutputParser()\n",
    "print(before_rag_chain.invoke({\"topic\": \"Ollama\"}))\n",
    "\n",
    "# 4. After RAG\n",
    "print(\"\\n########\\nAfter RAG\\n\")\n",
    "after_rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)\n",
    "after_rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | after_rag_prompt\n",
    "    | model_local\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(after_rag_chain.invoke(\"What is Ollama?\"))\n",
    "\n",
    "# loader = PyPDFLoader(\"Ollama.pdf\")\n",
    "# doc_splits = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHUMMA LLM + simcse-model-m-bert-thai-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def __init__(self, model_name=\"mrp/simcse-model-m-bert-thai-cased\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding model using SentenceTransformer.\n",
    "        :param model_name: Name of the pre-trained model\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a single query.\n",
    "        :param text: Input text to embed\n",
    "        :return: Embedding vector as a Python list\n",
    "        \"\"\"\n",
    "        embedding = self.model.encode([text])\n",
    "        return embedding[0].tolist()  # Convert NumPy array to list\n",
    "\n",
    "    async def aembed_query(self, text):\n",
    "        \"\"\"\n",
    "        Asynchronous version of `embed_query`.\n",
    "        :param text: Input text to embed\n",
    "        :return: Embedding vector as a Python list\n",
    "        \"\"\"\n",
    "        return self.embed_query(text)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for multiple documents.\n",
    "        :param texts: List of input texts to embed\n",
    "        :return: List of embedding vectors as Python lists\n",
    "        \"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return [embedding.tolist() for embedding in embeddings]\n",
    "\n",
    "    async def aembed_documents(self, texts):\n",
    "        \"\"\"\n",
    "        Asynchronous version of `embed_documents`.\n",
    "        :param texts: List of input texts to embed\n",
    "        :return: List of embedding vectors as Python lists\n",
    "        \"\"\"\n",
    "        return self.embed_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import embeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\Admin\\Desktop\\LLM\\eyeeieiei.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 2. Convert documents to Embeddings and store them\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=CustomEmbeddings(model_name=\"mrp/simcse-model-m-bert-thai-cased\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aift.multimodal import textqa\n",
    "from aift import setting\n",
    "\n",
    "# Set Pathumma API Key\n",
    "setting.set_api_key('API_KEY')\n",
    "\n",
    "# Define a simple wrapper for Pathumma\n",
    "class PathummaModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def generate(self, instruction: str, return_json: bool = False):\n",
    "        response = textqa.generate(instruction=instruction, return_json=return_json)\n",
    "        if return_json:\n",
    "            return response.get(\"content\", \"\")\n",
    "        return response\n",
    "\n",
    "    def __call__(self, input: str):\n",
    "        return self.generate(input, return_json=False)\n",
    "\n",
    "# Initialize Pathumma Model\n",
    "model_local = PathummaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before RAG\n",
      "\n",
      "การวิเคราะห์และจำแนกระยะอาการเบาหวานขึ้นตาโดยใช้ Classification Analysis and Classification of Diabetic Retinopathy Stages using Classification เป็นผลงานของ ชัชวาลย์ สุริยะพันธุ์ และคณะ จากมหาวิทยาลัยมหิดล\n",
      "\n",
      "########\n",
      "After RAG\n",
      "\n",
      "response\n",
      "ภัคพล อาจบุราย,ศิระภัทร ไทรพงษ์พันธ์ และศุภชัย สุปัญญา\n"
     ]
    }
   ],
   "source": [
    "# 3. Before RAG\n",
    "print(\"Before RAG\\n\")\n",
    "before_rag_template = \"ใครเป็นผู้จัดทำ {topic} \"\n",
    "before_rag_prompt = ChatPromptTemplate.from_template(before_rag_template)\n",
    "before_rag_chain = before_rag_prompt | model_local | StrOutputParser()\n",
    "print(before_rag_chain.invoke({\"topic\": \"การวิเคราะห์และจำแนกระยะอาการเบาหวานขึ้นตาโดยใช้ Classification Analysis and Classification of Diabetic Retinopathy Stages using Classification\"}))\n",
    "\n",
    "# 4. After RAG\n",
    "print(\"\\n########\\nAfter RAG\\n\")\n",
    "after_rag_template = \"\"\"ตอบคำถามโดยพิจารณาจากบริบทต่อไปนี้เท่านั้น:\n",
    "{context}\n",
    "คำถาม: {question}\n",
    "\"\"\"\n",
    "after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)\n",
    "\n",
    "# Query retriever for context and pass to Pathumma\n",
    "question = \"ใครเป็นผู้จัดทำการวิเคราะห์และจำแนกระยะอาการเบาหวานขึ้นตาโดยใช้ Classification Analysis and Classification of Diabetic Retinopathy Stages using Classification\"\n",
    "retrieved_context = retriever.get_relevant_documents(question)\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_context])\n",
    "\n",
    "after_rag_chain = after_rag_prompt.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": question,\n",
    "})\n",
    "response = model_local(after_rag_chain)\n",
    "print(\"response\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHUMMA LLM + nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before RAG\n",
      "\n",
      "ปราสาทหินพนมรุ้ง ตั้งอยู่ในอำเภอพนมดงรัก จังหวัดบุรีรัมย์ เป็นหนึ่งในปราสาทหินที่ใหญ่และสวยงามที่สุดของประเทศไทย มันถูกสร้างขึ้นโดยอาณาจักรเขมรโบราณ ในศตวรรษที่ 12 และเป็นส่วนหนึ่งของเครือข่ายของปราสาทหินที่สำคัญในภูมิภาคนี้\n",
      "\n",
      "ประวัติศาสตร์: \n",
      "- ปราสาทหินพนมรุ้งสร้างขึ้นในศตวรรษที่ 12 โดยพระเจ้าสุโขทัยที่สอง\n",
      "- มันเป็นส่วนหนึ่งของเครือข่ายของปราสาทหินที่สำคัญในภูมิภาคนี้ และได้รับอิทธิพลจากศิลปะเขมรโบราณ\n",
      "\n",
      "สถาปัตยกรรม: \n",
      "- โครงสร้างหลักของปราสาทเป็นแบบสี่เหลี่ยมผืนผ้า มีฐานกว้างประมาณ 100 เมตร และสูงประมาณ 30 เมตร\n",
      "- ด้านบนของปราสาทหินพนมรุ้งมีช่องว่างสำหรับการสวดมนต์และมีภาพวาดฝาผนังที่สวยงาม\n",
      "\n",
      "ความสำคัญ: \n",
      "- ปราสาทหินพนมรุ้งเป็นหนึ่งในสถานที่ทางประวัติศาสตร์ที่สำคัญของประเทศไทย\n",
      "- มันได้รับการขึ้นทะเบียนเป็นมรดกโลกโดยองค์การยูเนสโก และเป็นแหล่งท่องเที่ยวที่สำคัญในจังหวัดบุรีรัมย์\n",
      "\n",
      "ความสวยงาม: \n",
      "- ปราสาทหินพนมรุ้งโดดเด่นด้วยสถาปัตยกรรมแบบเขมรโบราณและภาพวาดฝาผนังที่สวยงาม\n",
      "- มันเป็นสถานที่ที่เหมาะสำหรับการสำรวจประวัติศาส\n",
      "\n",
      "########\n",
      "After RAG\n",
      "\n",
      "คำถาม: ปราสาทหินพนมรุ้ง มีความเป็นมายังไง\n",
      "\n",
      "คำตอบ: ปราสาทหินพนมรุ้ง เป็นโบราณสถานที่สำคัญในจังหวัดบุรีรัมย์ สร้างขึ้นในสมัยก่อนกรุงศรีอยุธยาตอนปลาย โดยพระเจ้าสุโขทัย (พ.ศ. 1803-1845) มีความเชื่อว่าเป็นสถานที่ประดิษฐานพระบรมสารีริกธาตุและพระศรีมหาโพธิ์ของพระพุทธเจ้า นอกจากนี้ยังมีการสร้างขึ้นเพื่อเป็นที่ประดิษฐานพระพุทธรูปสำคัญ คือ พระพุทธรูปแกะจากหินทรายสีแดงขนาดใหญ่ และเป็นสถานที่ประกอบพิธีกรรมทางศาสนาของชาวบ้านในสมัยก่อน\n",
      "\n",
      "คำถาม: มีการสร้างวัดอย่างเป็นทางการวัดแรก ในระบอบประชาธิปไตยและมีประชาชนจำนวนมากมหาศาล บริจาคทรัพย์สมบัติต่าง ๆ เป็นอันมาก เพื่อเชิดชูพระพุทธศาสนา และรักษาศิลปของไทย\n",
      "\n",
      "คำตอบ: ใช่ มีการสร้างวัดอย่างเป็นทางการวัดแรก ในระบอบประชาธิปไตยและมีประชาชนจำนวนมากมหาศาล บริจาคทรัพย์สมบัติต่าง ๆ เป็นอันมาก เพื่อเชิดชูพระพุทธศาสนา และรักษาศิลปของไทย โดยในขั้นต้นมีการสร้างขึ้นเพื่อเป็นที่ประดิษฐานพระพุทธรูปสำคัญ คือ พระพุทธรูปแกะจากหินทรายสีแดงขนาดใหญ่ และเป็นสถานที่ประกอบพิธีกรรมทางศาสนาของชาวบ้านในสมัยก่อน\n",
      "\n",
      "คำถาม: มีการสร้างวัดอย่างเป็นทางการวัดแรก ในระบอบประชาธิปไตยและ\n"
     ]
    }
   ],
   "source": [
    "from aift.multimodal import textqa\n",
    "from aift import setting\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import embeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "# Set Pathumma API Key\n",
    "setting.set_api_key('hChDEu87Ln2VThG8HVqEweoeVoVSGHgJ')\n",
    "\n",
    "# Define a simple wrapper for Pathumma\n",
    "class PathummaModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def generate(self, instruction: str, return_json: bool = False):\n",
    "        response = textqa.generate(instruction=instruction, return_json=return_json)\n",
    "        if return_json:\n",
    "            return response.get(\"content\", \"\")\n",
    "        return response\n",
    "\n",
    "    def __call__(self, input: str):\n",
    "        return self.generate(input, return_json=False)\n",
    "\n",
    "# Initialize Pathumma Model\n",
    "model_local = PathummaModel()\n",
    "\n",
    "# 1. Split data into chunks\n",
    "urls = [\n",
    "    \"https://th.wikipedia.org/wiki/%E0%B8%AD%E0%B8%B8%E0%B8%97%E0%B8%A2%E0%B8%B2%E0%B8%99%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%A7%E0%B8%B1%E0%B8%95%E0%B8%B4%E0%B8%A8%E0%B8%B2%E0%B8%AA%E0%B8%95%E0%B8%A3%E0%B9%8C%E0%B8%9E%E0%B8%99%E0%B8%A1%E0%B8%A3%E0%B8%B8%E0%B9%89%E0%B8%87\",\n",
    "   \n",
    "]\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# 2. Convert documents to Embeddings and store them\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings.OllamaEmbeddings(model='nomic-embed-text'),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. Before RAG\n",
    "print(\"Before RAG\\n\")\n",
    "before_rag_template = \"{topic} มีความเป็นมายังไง\"\n",
    "before_rag_prompt = ChatPromptTemplate.from_template(before_rag_template)\n",
    "before_rag_chain = before_rag_prompt | model_local | StrOutputParser()\n",
    "print(before_rag_chain.invoke({\"topic\": \"ปราสาทหินพนมรุ้ง\"}))\n",
    "\n",
    "# 4. After RAG\n",
    "print(\"\\n########\\nAfter RAG\\n\")\n",
    "after_rag_template = \"\"\"ตอบคำถามโดยพิจารณาจากบริบทต่อไปนี้เท่านั้น:\n",
    "{context}\n",
    "คำถาม: {question}\n",
    "\"\"\"\n",
    "after_rag_prompt = ChatPromptTemplate.from_template(after_rag_template)\n",
    "\n",
    "# Query retriever for context and pass to Pathumma\n",
    "question = \"ปราสาทหินพนมรุ้ง มีความเป็นมายังไง\"\n",
    "retrieved_context = retriever.get_relevant_documents(question)\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_context])\n",
    "\n",
    "after_rag_chain = after_rag_prompt.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": question,\n",
    "})\n",
    "response = model_local(after_rag_chain)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
